{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open ('outfile.txt', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)\n",
    "    \n",
    "print (len(itemlist[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28153\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "for sets in itemlist:\n",
    "    for item in sets:\n",
    "        elem = []\n",
    "        elem.extend(item['state'])\n",
    "        inputs.append(elem)\n",
    "        if item['won']:\n",
    "            labels.append(1.0 * item['move'] / 60.0)\n",
    "        else:\n",
    "            labels.append(-1.0 * item['move'] / 60.0)\n",
    "                          \n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "batch_size = 256 \n",
    "\n",
    "n_inputs = 7 * 7 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conneted_layer(inputs, output_len, name, stddev=0.1):\n",
    "    W1 = tf.get_variable(name + \"_weights\", shape=[int(inputs.get_shape()[-1]), output_len], initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "    b1 = tf.get_variable(name + \"_bias\", shape=[output_len], initializer=tf.constant_initializer(value=0.0))\n",
    "    return tf.matmul(inputs, W1) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(inputs):\n",
    "    fc1 = fully_conneted_layer(inputs, 512, 'fc_1', stddev = 0.02)\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    fc2 = fully_conneted_layer(fc1, 256, 'fc_2', stddev = 0.02)\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "    #fc2 = fully_conneted_layer(inputs, 256, 'fc_2', stddev = 0.02)\n",
    "    #fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "    out = fully_conneted_layer(fc2, 1, 'fc_3', stddev = 0.02)\n",
    "    #mout = tf.nn.sigmoid(logits)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_inputs])\n",
    "y = tf.placeholder(\"float\", [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "error = tf.subtract(mod, y)\n",
    "mse = tf.reduce_mean(tf.square(error))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "save_file = './model.ckpt'\n",
    "saver = tf.train.Saver()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features) == len(labels)\n",
    "    res = []\n",
    "    for i in range(0, len(features), batch_size):\n",
    "        size = min(batch_size, len(features) -  i)\n",
    "        features_slice = features[i : i + size]\n",
    "        labels_slice = labels[i : i + size]\n",
    "        res.append([features_slice, labels_slice])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28153\n",
      "Epoch 0   - mse: 0.10381358116865158\n",
      "Epoch 5   - mse: 0.10062479972839355\n",
      "Epoch 10  - mse: 0.08790715038776398\n",
      "Epoch 15  - mse: 0.0806150734424591\n",
      "Epoch 20  - mse: 0.07693179696798325\n",
      "Epoch 25  - mse: 0.0758780762553215\n",
      "Epoch 30  - mse: 0.06891506910324097\n",
      "Epoch 35  - mse: 0.061274558305740356\n",
      "Epoch 40  - mse: 0.058182016015052795\n",
      "Epoch 45  - mse: 0.046853676438331604\n",
      "Epoch 50  - mse: 0.0413372777402401\n",
      "Epoch 55  - mse: 0.03810950368642807\n",
      "Epoch 60  - mse: 0.03185166418552399\n",
      "Epoch 65  - mse: 0.03141096979379654\n",
      "Epoch 70  - mse: 0.023607127368450165\n",
      "Epoch 75  - mse: 0.027650058269500732\n",
      "Epoch 80  - mse: 0.026803454384207726\n",
      "Epoch 85  - mse: 0.017680397257208824\n",
      "Epoch 90  - mse: 0.015137093141674995\n",
      "Epoch 95  - mse: 0.018694967031478882\n",
      "Epoch 100 - mse: 0.015158242546021938\n",
      "Epoch 105 - mse: 0.013815158046782017\n",
      "Epoch 110 - mse: 0.012605990283191204\n",
      "Epoch 115 - mse: 0.015602700412273407\n",
      "Epoch 120 - mse: 0.014956174418330193\n",
      "Epoch 125 - mse: 0.01083381474018097\n",
      "Epoch 130 - mse: 0.014449894428253174\n",
      "Epoch 135 - mse: 0.012882955372333527\n",
      "Epoch 140 - mse: 0.009244920685887337\n",
      "Epoch 145 - mse: 0.011121914722025394\n",
      "Epoch 150 - mse: 0.010595424100756645\n",
      "Epoch 155 - mse: 0.007235952652990818\n",
      "Epoch 160 - mse: 0.008935107849538326\n",
      "Epoch 165 - mse: 0.011912800371646881\n",
      "Epoch 170 - mse: 0.009170925244688988\n",
      "Epoch 175 - mse: 0.011248827911913395\n",
      "Epoch 180 - mse: 0.008113940246403217\n",
      "Epoch 185 - mse: 0.00916315708309412\n",
      "Epoch 190 - mse: 0.007447970099747181\n",
      "Epoch 195 - mse: 0.009861140511929989\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        btchs = batches(batch_size, inputs, labels)\n",
    "        \n",
    "        for i in range(len(btchs)):\n",
    "            batch_x, batch_y = btchs[i]\n",
    "            batch_y = np.array(batch_y).reshape(-1, 1)\n",
    "            error, _ = session.run([mse, optimizer], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        if epoch % 5 == 0:\n",
    "            #valid_accuracy = session.run(accuracy, \\\n",
    "            #    feed_dict={ \\\n",
    "            #        x: mnist.validation.images, \\\n",
    "            #        y: mnist.validation.labels, \\\n",
    "            #               keep_prob: 1}) \n",
    "            print('Epoch {:<3} - mse: {}'.format( \\\n",
    "                epoch, \\\n",
    "                error)) \\\n",
    "       \n",
    "    tf_weights1 = tf.get_collection(tf.GraphKeys.VARIABLES, \"fc_1_weights\")[0]\n",
    "    tf_biases1 = tf.get_collection(tf.GraphKeys.VARIABLES, \"fc_1_bias\")[0]\n",
    "    tf_weights2 = tf.get_collection(tf.GraphKeys.VARIABLES, \"fc_2_weights\")[0]\n",
    "    tf_biases2 = tf.get_collection(tf.GraphKeys.VARIABLES, \"fc_2_bias\")[0]\n",
    "    tf_weights3 = tf.get_collection(tf.GraphKeys.VARIABLES, \"fc_3_weights\")[0]\n",
    "    tf_biases3 = tf.get_collection(tf.GraphKeys.VARIABLES, \"fc_3_bias\")[0]\n",
    "    \n",
    "    weights1 = tf_weights1.eval()\n",
    "    biases1 = tf_biases1.eval()\n",
    "    weights2 = tf_weights2.eval()\n",
    "    biases2 = tf_biases2.eval()\n",
    "    weights3 = tf_weights3.eval()\n",
    "    biases3 = tf_biases3.eval()\n",
    " \n",
    "    np.savetxt('w1.txt', weights1)\n",
    "    np.savetxt('b1.txt', biases1)\n",
    "    \n",
    "    np.savetxt('w2.txt', weights2)\n",
    "    np.savetxt('b2.txt', biases2)\n",
    "    \n",
    "    np.savetxt('w3.txt', weights3)\n",
    "    np.savetxt('b3.txt', biases3)\n",
    "   \n",
    "    saver.save(session, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-84c171a61f80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     test_accuracy = sess.run(\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         feed_dict={x: mnist.test.images, y: mnist.test.labels})\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy: {}'.format(accuracy.name))\n",
    "print('Hiiden input name: {}'.format(weights['hidden_layer'].name))\n",
    "print('Biases out name: {}'.format(biases['out'].name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
